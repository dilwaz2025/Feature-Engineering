{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment"
      ],
      "metadata": {
        "id": "7p6XKXu24h6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: What is a parameter?"
      ],
      "metadata": {
        "id": "p3-YFTwI4o2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters are the internal configurations of a machine learning model that determine how it processes input data to produce output. They are learned from the data during training and are crucial for the model's performance."
      ],
      "metadata": {
        "id": "eoIYRtH77e7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2:What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "neaCTeb47fnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It indicates how changes in one variable are associated with changes in another. The degree of correlation is quantified by the correlation coefficient, denoted as r, which ranges from -1 to +1: A negative correlation (also known as an inverse correlation) occurs when two variables move in opposite directions: as one increases, the other tends to decrease, and vice versa. This relationship is represented by a correlation coefficient r between 0 and -1."
      ],
      "metadata": {
        "id": "5UE_8FFW7oc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3:Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "hEOAC62u8BnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a subset of Artificial Intelligence (AI) that enables computers to learn from data and make decisions or predictions without explicit programming. Instead of following predefined rules, ML systems identify patterns in data and use these insights to improve their performance over time.\n",
        "Main Components of Machine Learning:\n",
        "A robust machine learning system comprises several key components that work together to process data and produce accurate outcomes:\n",
        "Data:\n",
        "Data serves as the foundation for ML models. It can be structured (like tables and spreadsheets), semi-structured (such as JSON files), or unstructured (like images, audio, and text). The quality and quantity of data directly influence the model's performance.\n",
        "Features:\n",
        "Features are individual measurable properties or characteristics of the data. For instance, in a dataset of houses, features might include square footage, number of bedrooms, and location. Feature engineering involves selecting, modifying, or creating new features to improve model accuracy.\n",
        "Algorithms:\n",
        "Algorithms are mathematical models or procedures that process data to identify patterns. Common ML algorithms include:\n",
        "Supervised Learning: Learns from labeled data to predict outcomes (e.g., linear regression, decision trees).\n",
        "Unsupervised Learning: Identifies hidden patterns in unlabeled data (e.g., clustering, association).\n",
        "Reinforcement Learning: Learns by interacting with an environment and receiving feedback through rewards or penalties.\n",
        "Model:\n",
        "A model is the output of an ML algorithm after training on data. It represents the learned patterns and is used to make predictions or decisions.\n",
        "Training:\n",
        "Training involves feeding data into an algorithm to allow the model to learn and adjust its parameters. This process continues iteratively to minimize errors and improve accuracy.\n",
        "Evaluation:\n",
        "After training, models are evaluated using metrics like accuracy, precision, recall, and F1 score to assess their performance.\n",
        "Optimization:\n",
        "Optimization techniques, such as gradient descent, are used to adjust the model's parameters to minimize the error or loss function, enhancing the model's predictive capabilities.\n",
        "Deployment:\n",
        "Once trained and evaluated, models are deployed into production environments where they can make real-time predictions or decisions.\n",
        "Monitoring and Maintenance:\n",
        "Continuous monitoring ensures the model performs well over time. Maintenance involves updating the model as new data becomes available or as performance degrades"
      ],
      "metadata": {
        "id": "rfdzHx_a8K7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4:How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "R3Kla7GK8bKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss value provides insights into the model's performance:\n",
        "Training Progress: A decreasing loss over time suggests that the model is learning effectively and improving its predictions.\n",
        "Model Comparison: Lower loss values typically indicate better-performing models, assuming other factors remain constant.\n",
        "Overfitting Detection: If the training loss continues to decrease while the validation loss increases, the model may be overfitting—learning the training data too well, including its noise, and failing to generalize to new data."
      ],
      "metadata": {
        "id": "aOHgDF7q8jjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kz-MvxiN8qij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5:What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "XVr8woMd8qxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Variables:\n",
        "Continuous variables are numerical variables that can take any value within a given range. They are measurable and can have an infinite number of possible values between any two points.\n",
        "Categorical Variables:\n",
        "Categorical variables (also known as qualitative variables) represent categories or groups. They can be further divided into two subtypes:\n",
        "Nominal Variables: These are categories without a natural order or ranking.\n",
        "Ordinal Variables: These categories have a meaningful order or ranking, but the intervals between the categories are not necessarily equal."
      ],
      "metadata": {
        "id": "8kjUO3758wjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6:How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "zUBCMJwu81gX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, as many algorithms require numerical input. Several encoding techniques are employed to transform categorical data into a format suitable for modeling. Here's an overview of the most common methods:\n",
        "1. Label Encoding\n",
        "Assigns a unique integer to each category. While simple, it introduces an ordinal relationship where none may exist, potentially misleading algorithms that interpret higher numbers as \"greater\" values.\n",
        "2. One-Hot Encoding\n",
        "Creates a new binary column for each category. Each observation is marked with a 1 in the column corresponding to its category and 0s elsewhere. This method avoids introducing any ordinal relationships.\n",
        "3. Ordinal Encoding\n",
        "Assigns integer values to categories based on their rank order. Unlike label encoding, ordinal encoding respects the inherent order of categories.\n",
        "4. Frequency Encoding\n",
        "Replaces each category with its frequency or count in the dataset. This method can be useful when the frequency of categories carries predictive information.\n",
        "5. Target Encoding (Mean Encoding)\n",
        "Replaces each category with the mean of the target variable for that category. This method can capture the relationship between the categorical feature and the target variable. However, it requires careful handling to avoid data leakage and overfitting.\n",
        "6. Binary Encoding\n",
        "Converts categories into binary numbers and splits the digits into separate columns. This method reduces dimensionality compared to one-hot encoding, making it suitable for high-cardinality features.\n",
        "7. Embeddings (for Deep Learning)\n",
        "Embeddings map each category to a dense vector in a continuous vector space. These vectors are learned during model training and can capture complex relationships between categories. Embeddings are particularly useful in deep learning models handling high-cardinality categorical variables."
      ],
      "metadata": {
        "id": "Rx9YfcNU86u-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7:What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "vYUstxJ09Go-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Dataset:\n",
        "Purpose: The model learns patterns, relationships, and structures from this data.\n",
        "Composition: Consists of input-output pairs (features and corresponding labels).\n",
        "Usage: Utilized to adjust model parameters through algorithms like gradient descent.\n",
        "Size: Typically comprises a significant portion of the total dataset (e.g., 70–80%).\n",
        "\n",
        "Testing Dataset:\n",
        "Purpose: Evaluates the model's performance on unseen data to assess its generalization ability.\n",
        "Composition: Contains data points not used during training, ensuring an unbiased evaluation.\n",
        "Usage: After training, the model's predictions are compared to actual outcomes to compute metrics like accuracy, precision, recall, etc.\n",
        "Size: Usually forms a smaller portion of the dataset (e.g., 20–30%)."
      ],
      "metadata": {
        "id": "MQSIlASF9Oyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8:What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "Mx4FhQ-o9UEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sklearn.preprocessing module in scikit-learn provides a suite of tools to transform raw data into formats suitable for machine learning algorithms. These preprocessing techniques are essential for improving model performance and ensuring that algorithms interpret the data correctly"
      ],
      "metadata": {
        "id": "0iVk3k_49gOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9:What is a Test set?"
      ],
      "metadata": {
        "id": "I-Ibh9mw9eZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, a test set is a subset of your dataset that is used exclusively to evaluate the performance of a trained model. It contains data that the model has never seen during training or validation, ensuring an unbiased assessment of how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "Sd2P267f9Dtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10:How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "zKSbirEA9vVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To effectively split data for model fitting and testing in Python, the train_test_split() function from the sklearn.model_selection module is commonly used.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming X and y are your features and target variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X: Features (input variables)\n",
        "y: Target variable (output)\n",
        "test_size=0.2: Allocates 20% of the data for testing and the remaining 80% for training\n",
        "random_state=42: Ensures reproducibility by controlling the shuffling of data\n",
        "This function randomly splits the dataset into training and testing subsets, which is crucial for evaluating the model's performance on unseen data.\n",
        "\n",
        "How to Approach a Machine Learning Problem:\n",
        "Approaching a machine learning problem systematically ensures effective model development and evaluation. Here's a structured workflow:\n",
        "\n",
        "Define the Problem: Determine whether it's a classification, regression, clustering, or other type of problem.\n",
        "TutorialsPoint\n",
        "\n",
        "Collect and Understand the Data: Gather relevant data and perform exploratory data analysis (EDA) to understand its structure, detect patterns, and identify any issues like missing values.\n",
        "\n",
        "Preprocess the Data: Handle missing values, encode categorical variables, normalize or standardize features, and split the data into training and testing sets.\n",
        "\n",
        "Select a Model: Choose an appropriate machine learning algorithm based on the problem type and data characteristics.\n",
        "\n",
        "Train the Model: Fit the model to the training data and tune hyperparameters to optimize performance.\n",
        "\n",
        "Evaluate the Model: Assess the model's performance using the test set and appropriate metrics (e.g., accuracy, precision, recall, RMSE).\n",
        "\n",
        "Iterate and Improve: Based on evaluation results, refine the model by adjusting features, tuning hyperparameters, or selecting a different algorithm.\n",
        "\n",
        "Deploy the Model: Once satisfied with the model's performance, deploy it to make predictions on new data."
      ],
      "metadata": {
        "id": "jvG5y9_b-Gx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11:Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "RdyIWQzZ-Z7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is crucial for several reasons:\n",
        "1. Understand Data Structure and Distribution\n",
        "EDA helps in understanding the dataset's structure, including the types of variables (e.g., categorical, numerical) and their distributions. This insight guides the selection of appropriate machine learning algorithms and preprocessing steps.\n",
        "2. Identify and Handle Missing or Erroneous Data\n",
        "Through EDA, you can detect missing values, duplicates, or incorrect entries. Addressing these issues early ensures that the model is trained on clean and accurate data, leading to better performance.\n",
        "3. Detect Outliers and Anomalies\n",
        "Identifying outliers during EDA allows you to decide whether to remove or adjust them. Outliers can skew model results, so it's essential to address them appropriately.\n",
        "4. Explore Relationships Between Variables\n",
        "EDA involves examining correlations and interactions between features. Understanding these relationships helps in feature selection and engineering, which can enhance model accuracy.\n",
        "5. Test Assumptions for Modeling\n",
        "Many machine learning algorithms have underlying assumptions (e.g., normality, linearity). EDA allows you to check these assumptions and transform data if necessary to meet them, ensuring the model's validity.\n",
        "6. Inform Feature Engineering and Selection\n",
        "Insights gained from EDA can guide the creation of new features or the selection of the most relevant ones, improving model performance and reducing complexity.\n",
        "7. Visualize Data for Better Understanding\n",
        "Visualization techniques like histograms, box plots, and scatter plots during EDA provide intuitive insights into the data, aiding in better decision-making and communication of findings.\n",
        "In summary, EDA serves as a foundational step in the machine learning workflow, ensuring that the data is well-understood, clean, and appropriately prepared for modeling."
      ],
      "metadata": {
        "id": "R-oICrHa-iq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12:What is correlation?"
      ],
      "metadata": {
        "id": "sqF73ewP-pTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables. It quantifies how changes in one variable correspond to changes in another."
      ],
      "metadata": {
        "id": "0FuMnPVj-voZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13:What does negative correlation mean?"
      ],
      "metadata": {
        "id": "mee1JHAy-zhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation refers to a statistical relationship between two variables where, as one variable increases, the other tends to decrease, and vice versa. This inverse relationship is quantified using a correlation coefficient, typically denoted as r, which ranges from -1 to +1:\n",
        "r = -1: Perfect negative correlation — every increase in one variable corresponds to a proportional decrease in the other.\n",
        "r = 0: No linear correlation — no predictable relationship between the variables.\n",
        "r = +1: Perfect positive correlation — both variables increase or decrease together in perfect proportion.\n",
        "In real-world scenarios, a negative correlation is often observed, though rarely perfect"
      ],
      "metadata": {
        "id": "dzvKylQV-49x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14:How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "EQMrMuEN_K-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the correlation between variables in Python, the most common approach is using the pandas library, which provides built-in functions for this purpose. Here's how you can do it:\n",
        "1. Using Pandas for Correlation\n",
        "The pandas.DataFrame.corr() method computes the Pearson correlation coefficient between numerical columns in a DataFrame\n",
        "2. Calculating Correlation Between Two Specific Columns\n",
        "If you're interested in the correlation between two specific columns, you can use the .corr() method directly on those columns:\n",
        "3. Visualizing the Correlation Matrix\n",
        "To better understand the relationships between variables, you can visualize the correlation matrix using a heatmap. The seaborn library is commonly used for this purpose:"
      ],
      "metadata": {
        "id": "KBjR5mVm_QOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15:What is causation? Explain difference between correlation and causation with an example."
      ],
      "metadata": {
        "id": "n4etaXJN_W1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation refers to a cause-and-effect relationship between two variables, where a change in one directly leads to a change in the other. In contrast, correlation indicates a statistical association between two variables, but it does not imply that one causes the other.\n",
        "Example: Ice Cream Sales and Drowning Incidents\n",
        "An observed correlation between increased ice cream sales and higher drowning incidents during summer months does not imply that eating ice cream causes drowning. The underlying factor is the warmer weather, which leads to both increased swimming activities and higher ice cream consumption. This illustrates that correlation does not necessarily indicate causation."
      ],
      "metadata": {
        "id": "MCOsoEao_bs4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16:What is an Optimizer? What are different types of optimizers? Explain each with an example"
      ],
      "metadata": {
        "id": "1jpsYgpY_nB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In machine learning, an optimizer is an algorithm used to minimize or maximize an objective function, typically a loss function, by adjusting the model's parameters (weights and biases). The goal is to find the optimal set of parameters that result in the best performance of the model.\n",
        "Types of Optimizers and Their Examples\n",
        "1. Stochastic Gradient Descent (SGD)\n",
        "Description: Updates model parameters using the gradient of the loss function with respect to the parameters, computed on a single training example.\n",
        "\n",
        "Use Case: Suitable for large datasets and online learning scenarios.\n",
        "\n",
        "Example: Training a simple neural network on a large dataset where computational efficiency is crucial.\n",
        "\n",
        "2. Momentum\n",
        "Description: Enhances SGD by adding a fraction of the previous update to the current update, helping to accelerate convergence and reduce oscillations.\n",
        "\n",
        "Use Case: Effective in navigating ravines in the error surface, common in deep learning models.\n",
        "\n",
        "Example: Training deep neural networks where the optimization landscape is complex.\n",
        "\n",
        "3. Nesterov Accelerated Gradient (NAG)\n",
        "Description: A variant of momentum that first makes a big jump along the momentum direction and then corrects the course using the gradient at the new position.\n",
        "\n",
        "Use Case: Provides a more responsive update, leading to faster convergence.\n",
        "\n",
        "Example: Training models where quick adaptation to changes in the gradient is\n",
        "\n",
        "4. Adagrad (Adaptive Gradient Algorithm)\n",
        "Description: Adjusts the learning rate for each parameter based on the historical sum of squares of the gradients.\n",
        "\n",
        "Use Case: Particularly useful for sparse data scenarios.\n",
        "\n",
        "Example: Training models on text data where features are sparse.\n",
        "\n",
        "\n",
        "5. RMSprop (Root Mean Square Propagation)\n",
        "Description: Modifies Adagrad by using a moving average of squared gradients to normalize the gradient, addressing the problem of rapidly diminishing learning rates.\n",
        "\n",
        "Use Case: Works well in non-stationary settings, such as training recurrent neural networks.\n",
        "\n",
        "Example: Training sequence models like LSTMs for tasks such as language modeling\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "Description: Combines the advantages of both momentum and RMSprop by computing adaptive learning rates for each parameter from estimates of first and second moments of the gradients.\n",
        "\n",
        "Use Case: Widely used in various deep learning tasks due to its efficiency and low memory requirements.\n",
        "\n",
        "Example: Training large-scale models like BERT for natural language processing tasks.\n",
        "\n",
        "\n",
        "7. AdaDelta\n",
        "Description: An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate by restricting the window of accumulated past gradients to some fixed size.\n",
        "\n",
        "Use Case: Addresses the problem of diminishing learning rates in Adagrad.\n",
        "\n",
        "Example: Training models where the learning rate needs to be more stable over time.\n",
        "\n",
        "8. Nadam (Nesterov-accelerated Adaptive Moment Estimation)\n",
        "Description: Combines Adam and NAG by incorporating Nesterov momentum into Adam.\n",
        "\n",
        "Use Case: Provides faster convergence in some cases by combining the benefits of Adam and NAG.\n",
        "\n",
        "Example: Training models where both adaptive learning rates and momentum are beneficial."
      ],
      "metadata": {
        "id": "FlkQhetd_tHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17:What is sklearn.linear_model?"
      ],
      "metadata": {
        "id": "OF7HJnxJ_2Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module in the **scikit-learn** (sklearn) library that provides various linear models for **supervised learning**, including regression and classification tasks. These models are based on linear relationships between input features and target variables."
      ],
      "metadata": {
        "id": "frUK8ianADB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18:What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "lUtdQEK5A2Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.fit() trains the model for a fixed number of epochs (iterations over the entire dataset). During this process, the model learns to map input features (x) to the correct outputs (y) by adjusting its internal parameters (weights) using optimization algorithms like gradient descent.\n",
        "\n",
        "Required Arguments:\n",
        "\n",
        "model.fit(x, y)\n",
        "\n",
        "x – Input data (e.g., images, sequences, tabular data, etc.)\n",
        "\n",
        "y – Target/output labels corresponding to x\n",
        "\n",
        "\n",
        "These are the only two required arguments."
      ],
      "metadata": {
        "id": "WglbdvIsBLC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19:What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "prECE7MyBLMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In libraries like Keras (TensorFlow), model.predict() is used to generate output predictions (inference) from the trained model for given input samples.\n",
        "\n",
        "It uses the learned weights (from training) to compute the predicted output based on the input features.\n",
        "\n",
        "\n",
        "Required Argument:\n",
        "\n",
        "model.predict(x)\n",
        "\n",
        "x – Input data you want predictions for (same shape as during training, except for the batch size).\n",
        "This is the only required argument."
      ],
      "metadata": {
        "id": "ed9-LEnsBLXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20:What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "ktfnuGnnBVrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are two fundamental types of variables used in statistics and machine learning:\n",
        "\n",
        "\n",
        "1. Continuous Variables:\n",
        "\n",
        "Definition: Variables that can take any value within a range (often infinite or very large).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Height (e.g., 172.5 cm)\n",
        "\n",
        "Weight (e.g., 68.3 kg)\n",
        "\n",
        "Temperature (e.g., 36.6°C)\n",
        "\n",
        "Income (e.g., $55,000.75)\n",
        "\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Numeric\n",
        "\n",
        "Can be measured\n",
        "\n",
        "Support arithmetic operations (add, multiply, etc.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Categorical Variables:\n",
        "\n",
        "Definition: Variables that represent categories or groups. They take on a limited, fixed number of values, often non-numeric (but can be encoded with numbers).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Gender (Male, Female, Other)\n",
        "\n",
        "Color (Red, Blue, Green)\n",
        "\n",
        "Education Level (High School, Bachelor, Master)\n",
        "\n",
        "Animal Type (Cat, Dog, Bird)\n",
        "\n",
        "\n",
        "Types:\n",
        "\n",
        "Nominal: No inherent order (e.g., color, gender)\n",
        "\n",
        "Ordinal: Have a natural order (e.g., low/medium/high)\n",
        "\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Often stored as strings or labels\n",
        "\n",
        "Cannot be measured, only counted or grouped\n",
        "\n",
        "Require encoding (e.g., one-hot encoding) for use in ML models"
      ],
      "metadata": {
        "id": "Om_UMHLTBVzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21:What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "7uIGsKYVBV69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is a technique used to normalize or standardize the range of independent variables (features) in your dataset so that they contribute equally to the learning process.\n",
        "\n",
        "\n",
        "\n",
        ">>Many machine learning algorithms compute distances (e.g., Euclidean distance) or use gradients during training. If your features have different scales (e.g., age in years vs. income in dollars), one feature can dominate others, leading to:\n",
        "\n",
        "Slower training\n",
        "\n",
        "Suboptimal models\n",
        "\n",
        "Unstable convergence\n",
        "\n",
        ">>Speeds up gradient descent (used in neural networks, logistic regression)\n",
        "\n",
        "Improves accuracy and convergence in models like:\n",
        "\n",
        "K-Nearest Neighbors (KNN)\n",
        "\n",
        "Support Vector Machines (SVM)\n",
        "\n",
        "Principal Component Analysis (PCA)\n",
        "\n",
        "Neural Networks"
      ],
      "metadata": {
        "id": "8rLGp_LFBV_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22:How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "R8KwbZIEBWFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can perform feature scaling in Python using libraries like scikit-learn (sklearn), which provides built-in scalers.\n",
        "\n",
        "\n",
        "\n",
        "Step-by-Step: Scaling with scikit-learn\n",
        "\n",
        "1. Import the scaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "2. Choose your data\n",
        "\n",
        "Example dataset:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[100, 0.5],\n",
        "              [200, 0.75],\n",
        "              [300, 1.0]])\n",
        "\n",
        "3. Apply a scaler\n",
        "\n",
        "Standard Scaling (mean = 0, std = 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "Min-Max Scaling (scales to [0, 1])\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "Robust Scaling (resistant to outliers)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "gWrYP8iSBWLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23:What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "NMeLUnRRBWXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sklearn.preprocessing module in scikit-learn provides a suite of tools to transform raw data into formats suitable for machine learning algorithms. These preprocessing techniques are essential for improving model performance and ensuring that algorithms interpret the data correctly"
      ],
      "metadata": {
        "id": "qzElewj-BvMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24:How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "3QL1MngZBvWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To effectively split data for model fitting and testing in Python, the train_test_split() function from the sklearn.model_selection module is commonly used.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming X and y are your features and target variables\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X: Features (input variables)\n",
        "y: Target variable (output)\n",
        "test_size=0.2: Allocates 20% of the data for testing and the remaining 80% for training\n",
        "random_state=42: Ensures reproducibility by controlling the shuffling of data\n",
        "This function randomly splits the dataset into training and testing subsets, which is crucial for evaluating the model's performance on unseen data."
      ],
      "metadata": {
        "id": "VUCvtV4bHRhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25:Explain data encoding?"
      ],
      "metadata": {
        "id": "yb5g6tE_B2sY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical (non-numeric) data into a numerical format that machine learning models can understand and process.\n",
        "\n",
        "Most ML algorithms (e.g., neural networks, decision trees, SVMs) cannot work directly with text or category labels—they need numbers.\n",
        "\n",
        "\n",
        "\n",
        "Why Encoding Is Needed:\n",
        "\n",
        "Machine learning models can only learn from numeric input. Categorical values like \"red\", \"blue\", \"male\", \"female\" must be translated into numbers without distorting their meaning.\n",
        "\n",
        "\n",
        "\n",
        "Common Types of Encoding:\n",
        "\n",
        "1. Label Encoding\n",
        "\n",
        "Converts each category into a unique integer.\n",
        "\n",
        "Example:\n",
        "\n",
        "Color: [Red, Blue, Green] → [0, 1, 2]\n",
        "\n",
        "Pros: Simple and fast\n",
        "\n",
        "Cons: Implies an order that may not exist (e.g., \"Green\" > \"Blue\" is meaningless)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. One-Hot Encoding\n",
        "\n",
        "Creates a binary column for each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "Color: [Red, Blue, Green] →\n",
        "\n",
        "       Red  Blue  Green\n",
        "       1     0     0\n",
        "       0     1     0\n",
        "       0     0     1\n",
        "\n",
        "Pros: No assumed order; safe for most algorithms\n",
        "\n",
        "Cons: Increases dimensionality if many categories\n",
        "\n",
        "\n",
        "\n",
        "3. Ordinal Encoding\n",
        "\n",
        "Like label encoding but used when the categories have a meaningful order.\n",
        "\n",
        "Example:\n",
        "\n",
        "Size: [Small, Medium, Large] → [0, 1, 2]\n",
        "\n",
        "\n",
        "\n",
        "4. Binary Encoding / Hash Encoding (Advanced)\n",
        "\n",
        "Useful for high-cardinality features (e.g., zip codes, URLs)\n",
        "\n",
        "Converts categories to binary format or hashed values to save space."
      ],
      "metadata": {
        "id": "Bz7oGT9jB6GD"
      }
    }
  ]
}